# 对比学习论文综述

对比学习在计算机视觉领域的发展历程

## 百花齐放

方法模型、目标函数、代理任务还没有统一的阶段，百花齐放的时代（18-19.6）

### Unsupervised Feature Learning via Non-Parametric Instance Discrimination

<!--提出实例判别和memory bank做对比学习-->

 [InstDisc.pdf](../../../学校/论文/InstDisc.pdf)

这篇论文讲到了==代理任务==（个体判别任务）

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 18.24.45.png" alt="截屏2022-04-10 18.24.45" style="zoom:50%;" />

方法受到有监督学习的启发，分类分数靠近的原因不是因为标签而是因为目标本身相似，根据这个观察提出了个体判别任务。

![截屏2022-04-10 18.27.17](/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 18.27.17.png)

文章的方法，通过一个卷积神经网络，把所有的图片都编码成特征，希望这些特征在最后的特征空间中能够尽可能得分开。

如何训练卷积神经网络？通过==对比学习==，所以需要有==正样本==与==负样本==，根据个体判别任务，正样本就是图片本身（可能通过数据增强），负样本就是数据集里所有其他的图片。

大量的负样本特征应该存在什么地方？==Memory Bank==，一个字典，由于图片数量多（128万），因此每个特征的维度不能太高，否则存储代价太大（128维）。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 18.38.52.png" alt="截屏2022-04-10 18.38.52" style="zoom:50%;" />

给模型的训练加了约束，从而可以得到动量式的更新。

### Unsupervised Embedding Learning via Invariant and Spreading Instance Feature

<!--一个编码器的端到端对比学习-->

 [InvaSpread.pdf](../../../学校/论文/InvaSpread.pdf)

SimCLR的前身，没有使用额外的数据结构去存储大量负样本，正负样本只来自同一个minibatch，而且只是用一个编码器，进行端到端的学习。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 18.45.19.png" alt="截屏2022-04-10 18.45.19" style="zoom:50%;" />

想法就是最基本的对比学习，同样的图片经过编码器（CNN）后特征应该很类似，不同的图片特征应该不类似（Invariant and Spreading）。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 18.46.52.png" alt="截屏2022-04-10 18.46.52" style="zoom:50%;" />

具体做法，在代理任务上也选取了个体判别任务，原始图片（256）经过数据增强又得到新图片，对于x1，x1'就是正样本，负样本为剩下所有图片。

为什么要从同一个MiniBatch中选择正负样本？可以用一个编码器进行端到端的训练。

### Representation Learning with Contrastive Predictive Coding

<!--对比预测编码，图像语音文本强化学习全都能做-->

 [CPC.pdf](../../../学校/论文/CPC.pdf) 

与前两篇不同，这篇文章用了生成式的代理任务。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 18.53.38.png" alt="截屏2022-04-10 18.53.38" style="zoom:50%;" />

一个很通用的结构，不光可以处理音频，还可以处理图片、文字以及在强化学习中使用。为了简单起见用了音频信号作为输入。有一个持续的序列（输入x），将t之前的时刻放入编码器，编码器返回特征，将特征输入自回归模型（gar, auto regressive）。每一步的输出ct（context representation），即上下文特征表示如果足够好，应该可以做出合理预测，所以用ct预测未来时刻的特征输出。

对比学习体现在？ 正样本为未来输入通过编码器以后得到的未来时刻特征输出，相当于作出的预测是Query，而真正未来时刻的输出是由输入决定的，因此相对于预测来说是正样本。负样本定义十分广泛，可以任意选取输入通过编码器得到输出，都应该与预测不相似。

### Contrastive Multiview Coding

<!--多视角下的对比学习-->

 [CMC.pdf](../../../学校/论文/CMC.pdf)

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 19.04.30.png" alt="截屏2022-04-10 19.04.30" style="zoom:50%;" />

定义正样本方式更为广泛，一个物体的很多视角都可以被当作正样本。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 19.19.33.png" alt="截屏2022-04-10 19.19.33" style="zoom:50%;" />

选取了NYU RGBD数据集，同时拥有四个视角，分别是原始图像、图像对应的深度信息、surface normal以及物体的分割图像。虽然不同的输入来自不同的传感器或不同模态，但是所有的输入对应的都是一张图片，一个东西，应该互为正样本（特征空间中的绿点），不配对视角的特征为负样本应该远离（红点）。

## CV双雄

发展非常迅速，工作之间只间隔一到两个月（19.6-20.6）

### Momentum Contrast for Unsupervised Visual Representation Learning

<!--无监督训练效果也很好-->

 [MoCov1.pdf](../../../学校/论文/MoCov1.pdf)  精读：[MoCo.pdf](MoCo.pdf) 

主要贡献：把之前对比学习的方法归纳总结成字典查询问题，提出==队列==与==动量编码器==。·

### A Simple Framework for Contrastive Learning of Visual Representations

<!--简单的对比学习 (数据增强 + MLP head + 大batch训练久)-->

 [SimCLRv1.pdf](../../../学校/论文/SimCLRv1.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 21.23.11.png" alt="截屏2022-04-10 21.23.11" style="zoom:50%;" />

如果有一个mini batch的图片（x）（N），对x里的所有图片做数据增强，同一个图片得到的两个图片互为正样本（N），负样本为剩下所有样本以及数据增强后的样本（2N-1）。有了正负样本之后，通过编码器（f）进行编码，由于两个编码器共享权重因此实际上只有一个编码器。如果是res50，得到的特征h有2048维。

SimCLR的一个重大创新点就是在特征后，加了一个projector（g），一个MLP层，得到128维的特征z。只在训练中使用，在下游任务中扔掉。

最后衡量正样本之间是否能达到最大一致性（Maximize agreement），采用了一个叫normalized temperature scaled的交叉熵函数，normalized是在特征后面进行了L2归一化，temperature scaled引入了温度。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 23.42.42.png" alt="截屏2022-04-10 23.42.42" style="zoom:50%;" />

SimCLR与InvaSpread的区别：

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 23.45.42.png" alt="截屏2022-04-10 23.45.42" style="zoom:50%;" />

更多的数据增强

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 23.46.26.png" alt="截屏2022-04-10 23.46.26" style="zoom:50%;" />

数据增强消融实验

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 23.47.43.png" alt="截屏2022-04-10 23.47.43" style="zoom:50%;" />

G函数

更大的batch size

### Improved Baselines with Momentum Contrastive Learning

<!--MoCov1 + improvements from SimCLRv1-->

 [MoCov2.pdf](../../../学校/论文/MoCov2.pdf) 

将SimCLR里的技术用到MoCo

### Big Self-Supervised Models are Strong Semi-Supervised Learners

<!--大的自监督预训练模型很适合做半监督学习-->

 [SimCLRv2.pdf](../../../学校/论文/SimCLRv2.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-10 23.58.16.png" alt="截屏2022-04-10 23.58.16" style="zoom:50%;" />

V1-V2:

更大的模型

MLP层变深（两层）FC RELU FC RELU

使用了动量编码器

### Unsupervised Learning of Visual Features by Contrasting Cluster Assignments

<!--聚类对比学习-->

 [SWaV.pdf](../../../学校/论文/SWaV.pdf) 

给定同样一张图片，如果去生成不同视角（views），希望可以用一个视角得到的特征去预测另一个视角的特征。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 00.04.33.png" alt="截屏2022-04-11 00.04.33" style="zoom:50%;" />

直接将所有图片的特征与特征做对比原始而且费资源，MoCo128W近似6W。借助先验信息，不做近似，不与大量负样本比，与聚类中心prototype（c）比。c矩阵，维度d（特征维度）*k（聚类中心，3000）。

得到z1与z2后，并不是直接在特征上做对比学习，而是先通过一个clustering的方法，让特征z和prototype c 生成目标（q1q2，ground truth）。

代理任务Swapped prediction：如果x1x2为正样本，那z1与z2的特征应该相似，应该可以互相去做预测，用z1与c做点乘应该可以预测q2，反之亦然，点乘之后的结果就为预测。

聚类的好处：相对于几万个负样本来说，3000个聚类中心小了很多。并且聚类中心拥有明确的语义含义，而之前如果只是随机抽样负样本的话，负样本可能还是正样本，而且有时候抽出来的负样本类别也不均衡。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 00.21.37.png" alt="截屏2022-04-11 00.21.37" style="zoom:50%;" />

结果显著。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 00.23.49.png" alt="截屏2022-04-11 00.23.49" style="zoom:50%;" />

主要的性能提升点来自一个trick，==Multi-crop==。之前对比学习的方法都是用两个crop（x1x2两个224*224正样本对），但是大crop抓住的是整个场景的特征，如果更想学习局部物体的特征？增加更多crop，但是模型复杂度提升。如何同时使用更多正样本而不增加复杂度？2个160+4个96 crop。

## 不用负样本

将所有方法归纳总结，卷积神经网络在对比学习的总结性工作

### Bootstrap Your Own Latent A New Approach to Self-Supervised Learning

<!--不需要负样本的对比学习-->

 [BYOL.pdf](../../../学校/论文/BYOL.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 09.46.24.png" alt="截屏2022-04-11 09.46.24" style="zoom:50%;" />

ftheta与fxi使用同样的网络架构但是参数不同，ftheta的参数随着梯度更新而更新，fxi与moco一样，用moving average的形式更新。 这里ztheta为256维。

之前的对比学习当得到ztheta和zxi后， 比如simclr，需要让他们尽可能得接近，即达到maximum agreement。但是BYOL没有这么做，而是加了一个predictor（qtheta），也是一个MLP，让预测与zxi尽可能一致，将原来的匹配问题变成预测问题。目标函数直接用了MSE loss。

没有负样本，模型训练怎么会不坍塌？

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 10.27.24.png" alt="截屏2022-04-11 10.27.24" style="zoom:50%;" />

https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/

博客的作者在复现BYOL的时候遗漏了一个==小细节==，从而导致模型训练不动，出现了模型坍塌的现象，这个小细节与==batch norm==有关。在说明发现之前，先看batchnorm带来了什么麻烦。

首先看projecttion head中的具体结构。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 10.36.58.png" alt="截屏2022-04-11 10.36.58" style="zoom:50%;" />

在SImCLR中，有一个图片通过编码器，得到embedding特征y（2048维），将y给gtheta（projection head，MLP），MLP中第一个全联接层维度为20482048，第二个维度为2048128，在MLP中有两个BN操作。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 10.36.47.png" alt="截屏2022-04-11 10.36.47" style="zoom:50%;" />

再看MoCov2， 它的MLP中没有BN。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 10.37.33.png" alt="截屏2022-04-11 10.37.33" style="zoom:50%;" />

BYOL中，gtheta与gxi都是protection head，qtheta是prediction head，三个东西用的都是同样的MLP结构，第一个全连接层后有BN而第二个后没有，就是这个小差别造成了这次的发现。博客的作者在基于MoCo的代码复现BYOL时没有加BN，模型的学习就坍塌了，因此他们进行了额外实验。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 10.40.58.png" alt="截屏2022-04-11 10.40.58" style="zoom:50%;" />

通过一系列对比消融实验的结果可以看出BYOL的训练不坍塌与BN在很大程度上有关系。 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 10.45.51.png" alt="截屏2022-04-11 10.45.51" style="zoom:50%;" />

训练不坍塌与BN的关系体现在何处？ BN操作将一个batch中的所有样本的特征拿过来算一下均值方差，也就是常说的running mean running variance，然后用整个batch算来的均值和方差做归一化。意味着做某个正样本的loss的时候，其实也看到了其他样本的特征，其实是存在信息泄漏的，在MoCo中通过Shuffling BN防止信息泄漏。因为有这种信息泄漏的存在，所以可以把batch中的其他样本当作成隐式的负样本，换句话说，当存在BN的时候，其实并不是只有正样本在自己与自己学习，其实也是在做对比，不过对比任务是当前的正样本图片与平均图片有什么差别，而这个平均图片就是BN产生的，与SWaV很像，BN生成的平均图片相当是聚类中心（mode）。

但是这就导致BYOL没有逃出对比学习的范畴，因此BYOL想通过实验看能不能找到另外一种解释为什么BYOL能够不坍塌。

 [BYOLBN.pdf](../../../学校/论文/BYOLBN.pdf) 

BYOL作者不想让大家觉得BYOL的成功依赖于BN，因此在 **BYOL works *even* without batch statistics** 中，作者做了一系列非常详细的实验。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 11.02.35.png" alt="截屏2022-04-11 11.02.35" style="zoom:50%;" />

作者通过一系列消融实验发现了几个现象。第一个现象是发现BN确实比较关键，因为没有BN的地方会发现SimCLR工作得很好，但是BYOL很差。但是通过完整的消融实验还发现了几个特例，正是这几个特例帮助作者找到了另外一个可以解释的理由。当Projector有BN而Predictor没有BN时，训练也失败了，如果BN真的很关键，如果真的能提供对比学习的隐式负样本，训练就不应该失败。当Encoder与Projector都没有使用BN时，SimCLR也失败了，意味着即使给了显式负样本也训练不出。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 11.12.57.png" alt="截屏2022-04-11 11.12.57" style="zoom:50%;" />

BYOL作者与博客作者达成了一个比较一致的结论：BN与设计初衷一样，主要目的就是为了帮助模型稳定训练，能够提高模型的训练稳健性，从而导致模型不会坍塌。BYOL作者又将这一结论进一步延伸，给出了一个可以解释的理由。如果一开始就能让模型初始化比较好的话，后面的训练即使离开了BN也没有问题。于是作者进行了另外的实验，用group norm（GN，归一化方式）和weight standardization（WS，模型初始化方式），发现训练结果也很好，作者最后强调GN与WS都没有计算批统计量（batch statistics），所以说这个版本的BYOL是没有做隐式对比的。 

### Exploring Simple Siamese Representation Learning

<!--化繁为简的孪生表征学习-->

 [SimSiam.pdf](../../../学校/论文/SimSiam.pdf) 

不需要负样本，不需要大的batch size，不需要动量编码器。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 11.27.38.png" alt="截屏2022-04-11 11.27.38" style="zoom:50%;" />

之所以叫siamese network（孪生网络），因为两个编码器的网络结构一样且共享参数，与BYOL唯一不一样的地方就是没有使用动量编码器。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 11.30.45.png" alt="截屏2022-04-11 11.30.45" style="zoom:50%;" />

伪代码十分简单。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 11.34.21.png" alt="截屏2022-04-11 11.34.21" style="zoom:50%;" />

最后作者得到的结论是之所以SimSiam能够成功训练，不会有模型坍塌，主要是因为有stop gradient这个操作的存在，随后作者还提出了一个假设，因为有stop gradient操作的存在，所以SImSiam的结构可以想象成EM算法，一个训练过程或者说一套模型参数实际上被人为地分成了两份，相当于在解决两个子问题，模型的更新也是在交替进行的。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 11.38.07.png" alt="截屏2022-04-11 11.38.07" style="zoom:50%;" />

作者在接下来进行了一些推导，其实到最后可以把它理解成一个k-means聚类问题，从这个角度来说SimSiam和SWaV也有一定的关系。于是作者在最后画了这么一张图，将所有孪生网络的做法归纳进行了总结和对比。

首先是SimCLR，因为是端到端的学习，所以两边都有梯度回传，但是它做的还是对比任务。SwAV做的也是对比任务，但是没有与负样本而是与聚类中心去对比，聚类中心来自于Sinkhorn-Knopp（SK）算法。BYOL有一个新的贡献，提出了Predictor，不再是对比任务而是变成了预测任务。最后的SimSaim整体与BYOL非常像，只不过是没有动量编码器。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 11.42.51.png" alt="截屏2022-04-11 11.42.51" style="zoom:50%;" />

最后是结果对比。

##  Transformer

Vis Transformer 的爆火

### An Empirical Study of Training Self-Supervised Vision Transformers

<!--如何更稳定的自监督训练ViT-->

 [MoCov3.pdf](../../../学校/论文/MoCov3.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 11.57.01.png" alt="截屏2022-04-11 11.57.01" style="zoom:50%;" />

MoCov2 + SimSaim

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 11.59.42.png" alt="截屏2022-04-11 11.59.42" style="zoom:50%;" />

问题出现：因为ViT的出现，作者将骨干网络由残差网络变为ViT，当batchsize比较小的时候，曲线比较平滑，不会出什么问题，当batchsize变大之后会出现抖动的情况。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 14.17.14.png" alt="截屏2022-04-11 14.17.14" style="zoom:50%;" />

针对这个问题，V3作者提出了一个小trick。作者首先观察了训练时每层回穿的梯度情况，发现每次loss大幅震动导致准确度大幅下降的时候，梯度也会有一个波峰，发生在第一层做patch projection（一个可以训练的全连接层）的时候。因此作者尝试不训练直接冻住全连接层，随机初始化了一个patch projection层，然后冻住，发现问题解决了。

### Emerging Properties in Self-Supervised Vision Transformers

<!--transformer加自监督在视觉也很香-->

 [DINO.pdf](../../../学校/论文/DINO.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 14.27.38.png" alt="截屏2022-04-11 14.27.38" style="zoom:50%;" />

 一个完全不用任何标签信息训练出的ViT，如果把自注意力图拿出来，进行可视化，发现能非常准确地抓住每个物体的轮廓，效果媲美语义分割。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 14.30.20.png" alt="截屏2022-04-11 14.30.20" style="zoom:50%;" />

自蒸馏框架。左边叫student网络，右边叫teacher网络。因为student要去预测输出，因此可以将teacher的输出作为ground truth。为了避免模型坍塌，引入了centering操作，将整个batch里的样本算一个均值，然后减掉这个均值。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 14.33.43.png" alt="截屏2022-04-11 14.33.43" style="zoom:50%;" />

伪代码与MoCov3也十分像。

## 最后的总结

 <img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-04-11 15.14.52.png" alt="截屏2022-04-11 15.14.52" style="zoom:50%;" />


# 视频理解论文串讲

2014-2021

综述论文 [A Comprehensive Study of Deep Video Action Recognition.pdf](../../../学校/论文/A Comprehensive Study of Deep Video Action Recognition.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 15.58.13.png" alt="截屏2022-05-03 15.58.13" style="zoom: 50%;" />

## Hand-Crafted -> CNN

### Deep Video

Large-scale Video Classification with Convolutional Neural Networks

<!--CVPR 14-->

 [DeepVideo.pdf](../../../学校/论文/DeepVideo.pdf) 

AlexNet出现之后，深度学习时代视频理解最早工作之一。如何把卷积神经网络从图片识别应用到视频识别。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.06.00.png" alt="截屏2022-05-03 16.06.00" style="zoom:50%;" />

视频和图片的唯一区别就是多了一个时间轴。因此有许多变体需要尝试（图中后三个）。

**Single Frame**：单帧方式，图片分类任务。在视频里任选一帧得到结果，作为baseline。完全没有时间信息与视频信息在其中。

**Late Fusion**：在网络输出层面做的融合。在视频中随机选择几帧，每一帧单独通过一个卷积神经网络（权值共享），将得到的特征合并通过FC得到输出。由于将特征合并因此稍微有一些时序上的信息在里面。

**Early Fusion**：在输入层面进行融合。将五个视频帧在RGB通道上直接进行融合，由原来的一张图片3个Channel变成五个图片15个channel。意味着网络结构要进行改变。在输入层面感受时序上的改变。

**Slow Fusion**：Late太晚，Early太早，在特征层面进行融合。每次选10个视频帧的小视频段，每4个视频帧通过一个卷积神经网络，抽取一些特征。

预训练后迁移学习的效果比不上之前的手工特征，因此作者进行了另外一种尝试。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.18.11.png" alt="截屏2022-05-03 16.18.11" style="zoom:50%;" />

**多分辨率的卷积神经网络结构**。2DCNN学习时序特征确实很难，将图像CNN中好用的trick应用到视频领域。将输入分成两部分：原图（context stream）与原图中扣出的一小部分（fovea stream，不论对图片还是视频，一般最有用的或者物体都会出现在图片的正中间）。也可以理解成双流结构，不过两个网络权值共享。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.23.14.png" alt="截屏2022-05-03 16.23.14" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.23.34.png" alt="截屏2022-05-03 16.23.34" style="zoom:50%;" />

效果远不如手工特征的结果。为什么深度学习在视频领域碰壁？

## Two-Stream

Two-Stream Convolutional Networks for Action Recognition in Videos

 [Two Stream.pdf](../../../学校/论文/Two Stream.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.27.17.png" alt="截屏2022-05-03 16.27.17" style="zoom:50%;" />

当用一个卷积神经网络无法很好地处理时序信息的时候，只需要再加一个专门处理时序信息的卷积神经网络。比如从原始视频信息中抽取光流图像。两个网络各司其职。最后进行Late Fusion。

提出的研究方向：Early Fusion？网络结构（如何在小数据集上训练大模型是难点，如何控制过拟合）？RNN/LSTM处理时序信息？如何做长时间的视频理解？

### LSTM处理时序信息？

Beyond **Short Snippets**: Deep Networks for Video Classification

<!--CVPR 2015-->

[Beyond-short-snippets.pdf](../../../学校/论文/Beyond-short-snippets.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.33.24.png" alt="截屏2022-05-03 16.33.24" style="zoom:50%;" />

如果按照原始双流网络的思想去做，能处理的视频其实非常短（10帧），如何处理特别多的视频帧？

关键在于通过网络抽取到特征之后如何进行pooling的操作。同时尝试了使用LSTM进行特征融合。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.37.51.png" alt="截屏2022-05-03 16.37.51" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.38.49.png" alt="截屏2022-05-03 16.38.49" style="zoom:50%;" />

### Early Fusion

Convolutional Two-Stream Network **Fusion** for Video Action Recognition

<!--CVPR 16-->

[Convolutional fusion.pdf](../../../学校/论文/Convolutional fusion.pdf) 

详细地讲了如何进行合并。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.46.16.png" alt="截屏2022-05-03 16.46.16" style="zoom:50%;" />

如何在时间流与空间流之间进行Early Fusion？如何将特征图上对应的点进行合并（Spatial Fusion）？在哪一层进行fusion？

**Spatial Fusion**：拥有时间流与空间流两个网络之后，如何保证两个流的特征图在同样的位置上产生的通道response是差不多能联系起来的，这样才算是Early Fusion（在特征图层面进行合并）。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.53.51.png" alt="截屏2022-05-03 16.53.51" style="zoom:50%;" />

作者进行了几个简单的尝试，max fusion指对于a和b两个不同的特征图，在同样的位置上只取最大值作为合并之后的值，Concatenation fusion直接合并两个特征图，Conv fusion先堆叠两个特征图随后进行卷积操作，sum fusion直接进行加法，Bilinear fusion最复杂，但并不是表现最好的。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 16.59.01.png" alt="截屏2022-05-03 16.59.01" style="zoom:50%;" />

在空间维度上已经知道如何处理两个网络的特征图了，接下来的问题就是在哪个部分进行合并？作者进行了大量的消融实验，得到了两个比较好的方式。

第一种就是空间流与时间流分别做，最后在conv4进行fusion，fusion完后变成一个网络（传统Early fusion）。另外一个表现更好的方式是先分别做，在conv5的时候进行fusion，得到一个 spatiotemporal的特征（既有空间信息也有时间信息），同时空间流并没有完全抛弃（保持了空间流的完整性），最后进行合并得到分类结果。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 17.19.07.png" alt="截屏2022-05-03 17.19.07" style="zoom:50%;" />

如何进行temporal fusion，即如何在时间轴维度上进行合并得到最后的特征？3D pooling / 3D Conv + 3D pooling

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-03 17.20.04.png" alt="截屏2022-05-03 17.20.04" style="zoom:50%;" />

最后架构。 蓝色代表空间流，绿色代表时间流。抽取好特征后在Conv5先对空间流与时间流进行early fusion。最后Late fusion。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 14.45.54.png" alt="截屏2022-05-04 14.45.54" style="zoom:50%;" />

### 如何做长时间视频理解？

Temporal Segment Networks: **Towards Good Practices for Deep Action Recognition**

<!--ECCV 2016-->

 [TSN.pdf](../../../学校/论文/TSN.pdf) 

贡献完全不逊色于双流网络或I3D网络。确定了一些很好用的技巧（如何数据增强、初始化、如何使用光流、网络、如何防止过拟合）。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 14.52.55.png" alt="截屏2022-05-04 14.52.55" style="zoom:50%;" />

将视频分成多段，在每一段中随机抽取1帧，处理长时间视频。

好用的技巧：

- ==Cross Modality Pre-training==： Modality指图像和光流，可以理解成多模态。图像方面有ImageNet数据集做预训练，可以直接拿来微调，但是光流却没有很大数据集进行预训练。如何有效预训练光流？将ImageNet预训练好的用给光流。不过ImageNet预训练好的模型接收的输入是RGB图像，3个Channel，而光流的输入是10张光流图，每个光流图2个Channel（x方向与y方向分别的位移），总共20个Channel，如何将3个Channel变成20个Channel使用在光流上？修改预训练模型第一个卷积层的参数，将RGB3个Channel的参数做平均变成一个Channel，再将一个通道复制20次变成20个通道。
- Regularization Techniques：BN虽然能让训练加速，但是同时带来了很严重的过拟合的问题。作者提出了==Partial BN==，只打开第一层的BN，将后面的全部冻住（we choose to freeze the mean and variance parameters of all Batch Normalization layers except the first one），来适应新的输入，后面的不动是为了避免过拟合的风险。
- Data Augmentation：为了防止过拟合，数据增强是一个必不可少的工具。作者提出了==corner cropping==与==scale-jittering==。强制性在图片的边角进行裁剪，与改变图片的长宽比，使图片变得多样性。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 15.16.48.png" alt="截屏2022-05-04 15.16.48" style="zoom:50%;" />

## 中间总结

![image-20220504152308138](/Users/hanyixiao/Library/Application Support/typora-user-images/image-20220504152308138.png)

## 3D CNN

抛弃双流网络：光流抽取需要的时间十分久，预处理耗时且耗费空间，并且在推理时无法做到实时。

Learning Spatiotemporal Features with 3D Convolutional Networks

<!--ICCV 15-->

 [C3D.pdf](../../../学校/论文/C3D.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 15.32.23.png" alt="截屏2022-05-04 15.32.23" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 15.32.46.png" alt="截屏2022-05-04 15.32.46" style="zoom:50%;" />

3D版本的VGG。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 15.34.53.png" alt="截屏2022-05-04 15.34.53" style="zoom:50%;" />

16个视频帧。第一层Pooling是1x2x2，在时序上不做下采样，因为想尽可能多地保留时序信息。C3D指的更多是FC6抽取出的特征，叫做C3D特征。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 15.39.06.png" alt="截屏2022-05-04 15.39.06" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 15.40.40.png" alt="截屏2022-05-04 15.40.40" style="zoom:50%;" />

Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset

 [I3D.pdf](../../../学校/论文/I3D.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 15.46.04.png" alt="截屏2022-05-04 15.46.04" style="zoom:50%;" />

直接将2D网络扩充成3D网络，并且能够使用2D网络预训练好的参数。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 15.49.14.png" alt="截屏2022-05-04 15.49.14" style="zoom:50%;" />

**Non-local** Neural Networks

 [Non-local.pdf](../../../学校/论文/Non-local.pdf) 

在C3D与I3D之后，3D网络的基本结构已经确定，接下来就是做各式改进。时序建模或如何处理更长的时序。

将self-attention替代LSTM。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 15.57.24.png" alt="截屏2022-05-04 15.57.24" style="zoom:50%;" />

标准的自注意力模块。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 15.58.46.png" alt="截屏2022-05-04 15.58.46" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.03.11.png" alt="截屏2022-05-04 16.03.11" style="zoom:50%;" />

A Closer Look at Spatiotemporal Convolutions for Action Recognition

<!--CVPR 2018-->

 [R2+1D.pdf](../../../学校/论文/R2+1D.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.09.09.png" alt="截屏2022-05-04 16.09.09" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.08.57.png" alt="截屏2022-05-04 16.08.57" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.10.23.png" alt="截屏2022-05-04 16.10.23" style="zoom:50%;" />

比3D网络好的原因：有效地增强了模型的非线性能力。并且更容易学习。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.13.03.png" alt="截屏2022-05-04 16.13.03" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.16.17.png" alt="截屏2022-05-04 16.16.17" style="zoom:50%;" />

SlowFast Networks for Video Recognition

<!--ICCV 2019-->

 [SlowFast.pdf](../../../学校/论文/SlowFast.pdf) 

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.19.28.png" alt="截屏2022-05-04 16.19.28" style="zoom:50%;" />

人的视觉系统P细胞（数量多，80%，处理静态图像）和M细胞（数量少，20%，处理运动信息）。

用很低的帧率（每隔16帧取1帧，图中共取了4帧），叫做慢分支（P细胞），用来学习静态图像，场景信息。大部分的模型参数给到了慢分支，网络结构很大，一个I3D网络。用很高的帧率（每隔4帧，取到16帧），给快分支（M细胞），网络结构小。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.25.11.png" alt="截屏2022-05-04 16.25.11" style="zoom:50%;" />

在时序上没有进行下采样。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.27.41.png" alt="截屏2022-05-04 16.27.41" style="zoom:50%;" />

## Video transformer

Is Space-Time Attention All You Need for Video Understanding?

 [Timesformer.pdf](../../../学校/论文/Timesformer.pdf) 

如何将ViT从图像迁移到视频领域。

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.31.29.png" alt="截屏2022-05-04 16.31.29" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-05-04 16.36.50.png" alt="截屏2022-05-04 16.36.50" style="zoom:50%;" />


# 9.2 HPO algorithms

## Search Space

Specify range for each hyperparameter

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-20 20.51.38.png" alt="截屏2021-12-20 20.51.38" style="zoom:50%;" />

The search space can be exponentially large

- Need to carefully design the space to improve efficiency

  ## HPO algorithms: Black-box or Multi-fidelity

Black-box: treats a training job as a black-box in HPO:

- Completes the training process for each trial

Multi-fidelity: modifies the training job to speed up the search

- Train on subsampled datasets
- Reduce model size (e.g. less #layers, #channels)
- Stop bad configuration earlier 

## HPO algorithms

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-20 21.42.39.png" alt="截屏2021-12-20 21.42.39" style="zoom:50%;" />

## Two most common HPO strategies

### Grid search

```python
for config in search_space:
  train_and_eval(config)
return best_result
```

All combinations are evaluated

Guarantees the best results

Curse of dimensionality

### Random search (useful)

```python
for _ in range(n):
  config = random_select(search_space)
  train_and_eval(config)
return best_result
```

Random combinations are tried

More efficient than grid search (empirically and in theory, shown in <u>Random Search for Hyper-Parameter Optimization</u>)

## Bayesian Optimization (BO)

==BO==: Iteratively learn a mapping from HP to objective function.

Based on previous trials.Select the next trial based on the current estimation.

==Surrogate model==

- Estimate how the objective function depends on HP
- Probabilistic regression models: Random forest, Gaussian process

## Successive Halving

Save the budget for most promising config

Randomly pick n configurations to train m epochs

Repeat until one configuration left:

- Keep the best n/2 configuration to train another m epochs
- Keep the best n/4 configuration to train another 2m epochs
- ...

Select n and m based on training budget and #epoch needed for a full training

## Hyperband

In Successive Halving

- n: exploration
- m: exploration

Hyperband runs multiple Successive Halving, each time decreases n and increases m

- More exploration first, then do more exploit
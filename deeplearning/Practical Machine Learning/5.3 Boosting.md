# 5.3 Boosting

Boosting combines weak learners into a strong one

- Primarily to ==reduce bias==

Learn $n$ weak learners sequentially, at step $i$ :

- Train a weak learner $h_i$ evaluate its errors $\epsilon_t$
- Re-sample data according to $\epsilon_t$ to focus on wrongly predicted examples

Notable examples include AdaBoost, gradient boosting

## Gradient Boosting

Denote by $H_t(x)$ the model at time $t_1$, with $H_1(x)=0$

At step $t = 1,2,...$

- Train a new model $h_t$ on residuals: $\{(x_i,y_i-H_t(x_i)\}_{i=1,...,m}$
- $H_{t+1}(x)=H_t(x)+\eta h_t(x)$
  - The learning rate $\eta$ regularizes the model by shrinkage

The residuals equal to $-\delta L /\delta H$ if using MSE as the loss 

## Gradient Boosting Code

```python
class GradientBoosting:
  def __init__(self, base_learner, n_learners, learning_rate):
    self.learners = [clone(base_learner) 
                     for _ in range(n_learners)]
    self.lr = learning_rate
    
  def fit(self, X, y):
    residual = y.copy()
    for learner in self.learners:
      learner.fit(X, residual)
      residual -= self.lr * learner.predict(X)
      
  def predict(self, X):
    preds = [learner.predict(X) for learner in self.learners]
    return np.array(preds).sum(axis=0) * self.lr
```

## Gradient Boosting Decision Trees (GBDT)

Use decision tree as the weak learner

- Regularize by a small max_depth and randomly sampling features

Sequentially constructing trees runs slow

- Popular libraries use accelerated algorithms, e.g. ==XGBoost==, ==lightGBM==

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-04 20.39.33.png" alt="截屏2021-12-04 20.39.33" style="zoom:50%;" />

## Summary

Boosting combines weak learners into a strong one to reduce bias

Gradient boosting learns weak learners by fitting the residuals
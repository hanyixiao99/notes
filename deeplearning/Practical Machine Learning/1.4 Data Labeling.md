# 1.4 Data Labelling

## Flow Chart for Data Labelling

![截屏2021-11-22 16.30.28](/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-11-22 16.30.28.png)

## Semi-Supervised Learning (SSL)

- Focus on the scenario where there is a small amount of labeled data, along with large amount of unlabeled data
- Make assumptions on data distribution to use unlabeled data
  - ***Continuity assumption*** (连续性假设)
  - ***Cluster assumption*** (聚类假设)
  - ***Manifold assumption*** (流形假设)

## Self-training

- Self-training is a SSL methon
- We can use expensive models
  - Deep neural networks, model ensemble/bagging

## Label through Crowdsourcing

- ImageNet labeled millions of images through Amazon Mechanical Turk

### Challenges

- Simplify user interaction
- Cost
- Quality control

### User interaction
pass

### Reduce #tasks:Active Learning

- Focus on same scenario as SSL but with human intervention
- ***Uncertainty Sampling*** chooses an example whose prediction is most uncertain
  - The highest class prediction score is close to random (1/n)
- Similar to self-training we can use expensive models
  - ***Query-by-committee*** trains multiple models and performs major voting

### Active Learning + Self-training

These two methods are often used together 

### Quality Control

pass

## Weak Supervision

-  Semi-automaticaly generate labels

  - Less accurate than manual ones, but good enough for training

- ***Data programming***:heuristic programs to assign labels

  - Keyword search, pattern matching, third-party models

  - E.g. rules to check if YouTube comments are spam or ham

    ```python
    def check_out(x):
      return SPAM if "check out" in x.lower()
    																else ABSTAIN
    def sentiment(x):
      return HAM if sentiment_polarity(x) > 0.9
    																else ABSTAIN
    def short_comment(x):
      return HAM if len(x.split()) < 5 
    																else ABSTAIN
    ```

## Summary

- Way to get labels
  - Self-training
  - Crowdsourcing
  - Data programming
- Alternatively, You could also consider unsupervised/self-supervised learnings 

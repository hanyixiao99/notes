# 9.1 Model Tuning

## Manual Hyperparameter Tuning

Start with a good baseline, e.g. default settings in high-quality toolkits, values reported in papers

Tune a value, retrain the model to see the changes

Repeat multiple times to gain insights about

- Which hyperparamaters are important 
- How sensitive the model to hyperparamaters
- What are the good ranges

Needs careful experiment management

Save your training logs and hyperparameters to compare, share and reproduce later

- The simplest way is saving logs in text and put key metrics in Excel
- Better options exist, e.g. ==tensorboard== and ==weights & bias==

Reproducing is hard, it relates to

- Environment(hardware & library)
- Code
- Randomness(seed)

## Automated Hyperparamater Tuning

Computation costs decrease exponentially, while human costs increase

Cost per training for a typical ML task:

- E.g. 1M user logs, 10K images

Cost of data scientist per day >$500

Use algorithms if it outperforms human after 1000 trials

- Typically beat 90% data scientists

## Automated Machine Learning (AutoML)

Automate every step in applying ML to solve real-world problems: data cleaning, feature extraction, model selection...

==Hyperparameter optimization (HPO)==: find a good set of hyperparameters through search algorithms

==Neural architecture search (NAS)==: construct a good neural network model
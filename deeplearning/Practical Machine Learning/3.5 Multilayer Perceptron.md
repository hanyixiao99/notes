# 3.5 Multilayer Perceptron

## Linear Methods to MLP

A dense (fully connected, or linear) layer has parameters ***W***,***b***

it computes output***y = Wx + b***

- Linear regression: dense layer with 1 output
- Softmax regression: dense layer with ***m***outputs + softmax

### MLP

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-01 16.21.56.png" alt="截屏2021-12-01 16.21.56" style="zoom: 25%;" />

Activation is a elemental-wise ==non-linear== function

- $ sigmoid(x)=\frac{1}{1+exp(-x)},ReLU(x)=max(x,0) $
- It leads to non-linear models

Stack multiple hidden layers (dense + activation) to get deeper models

Hyper-parameters

- hidden layers
- outputs for each hidden layer

## Code (with PyTorch)

MLP with 1 hidden layer

Hyperparameter: num_hiddens

```python
def relu(X):
  return torch.max(X, 0)

W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens))
W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs))

H = relu(X @ W1 + b1) # non-linear function
Y = H @ W2 + b2
```


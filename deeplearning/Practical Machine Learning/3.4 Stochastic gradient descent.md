# 3.4 Stochastic gradient descent

Train by mini-batch SGD (by various other ways as well)

- Pros: solve all objectives in this course except for trees

- Cons: sensitive to hyper-parameters b and rate t

## Code

Full code: http://d2l.ai/chapter_linear-networks/linear-regression-scratch.html

Train a linear regression model with mini-batch SGD

Hyper-parameters

- Batch_size
- Learning_rate
- Num_epochs

```python
# features shape is (n, p), labels shape is (p, 1)
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # The examples are read at random, in no particular order
    random.shuffle(indices)
#   Mxnet:
    for i in range(0, num_examples, batch_size):
        batch_indices = np.array(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
        
#   Pytorch:        
#   for i in range(0, num_examples, batch_size):
#       batch_indices = torch.tensor(
#           indices[i: min(i + batch_size, num_examples)])
#       yield features[batch_indices], labels[batch_indices]

#   Tensorflow:
#   for i in range(0, num_examples, batch_size):
#       j = tf.constant(indices[i: min(i + batch_size, num_examples)])
#       yield tf.gather(features, j), tf.gather(labels, j)

# pass else
```


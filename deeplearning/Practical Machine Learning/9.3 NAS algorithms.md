# 9.3 NAS algorithms

## Nerual Architecture Search (NAS)

A neural network has different types of hyperparameters:

- Toplogical structure: resent-ish, mobile net-ish, #layers
- Individual layers: kernel_size, #channels in convolutional layer, #hidden_outputs in dense/recurrent layers

NAS automates the design of neural network

- How to specify the search space of NN
- How to explore the search space 
- Performance estimation

## NAS with Reinforcement Learning

Zoph & Le 2017

- A RL-based controller (REINFORCE) for proposing architecture
- RNN controller outputs a sequence of tokens to config the model architecture 
- Reward is the accuracy of a sampled model at convergence

Naive approach is expensive and sample inefficient (~2000 GPU days). 

To speed up NAS:

- Estimate performance
- Parameter sharing (e.g. EAS, ENAS)

## The One-shot Approach

Combines the learning of architecture and model params

Construct and train a single model presents a wide variety of architectures

Evaluate candidate architectures

- Only care about the candidate ranking
- Use a proxy metric: the accuracy after a few epochs

Re-train the most promising candidate from scratch

## Differentiable Architecture Search


# 5.4 Stacking

Combine multiple base learners to reduce variance

- Base learners can be different model types
- Linearly combine base learners outputs by learned parameters

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-04 20.50.05.png" alt="截屏2021-12-04 20.50.05" style="zoom:50%;" />

Widely used in ==competitions==

In comparson, bagging

- Uses same type models
- Uses booststrap to get diversity

## Stacking Resluts

Evaluate on house sales data. compare to bagging and GBDT we implemented before

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-04 20.55.05.png" alt="截屏2021-12-04 20.55.05" style="zoom: 50%;" /><img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-04 20.57.12.png" alt="截屏2021-12-04 20.57.12" style="zoom: 33%;" />

## Multi-layer Stacking

Stacking base learners in multiple levels to reduce bias

- Can use a different set of base learners at each level

Upper levels (e.g. L2) are trained on the outputs of the level below (e.g. L1)

- Concatenation original inputs helps

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-04 20.59.20.png" alt="截屏2021-12-04 20.59.20" style="zoom: 33%;" />

## Overfitting in Multi-layer Stacking

Train leaners from different levels on different data to alleviate overfitting

- Split training data into A and B, train L1 learners on A, predict on B to generate inputs to L2 learners

Repeated $k$-fold bagging:

- Train $k$ models as in $k$-fold cross validation
- Combine predictions of each model on out-of-fold data
- Repeat step 1,2 by $n$ times, average the $n$ predictions of each example for the next level training

## Multi-layer Stacking Results

Use 1 additional staked level, with 5-fold repeated bagging

- Error: 0.229 to 0.227
- Training time: 39 sec to 207 sec (5x)

```python
from autogluon.tabular import TabularPredictor

predictor = TabularPredictor(label=label).fit(
						train, num_stack_levels=1, num_bag_folds=5)
```

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-04 21.16.01.png" alt="截屏2021-12-04 21.16.01" style="zoom:50%;" />

## Summary

Stacking combine multiple learners to reduce variance

Stacking learners in multiple levels to reduce bias

- Repeatd $k$-fold bagging: fully utilize data and alleviate overfitting

## Model Combination Summary

The goal is to reduce bias and variance

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-04 21.20.22.png" alt="截屏2021-12-04 21.20.22" style="zoom:50%;" />
# 3.7 Recurrent Neural Network

## Dense layer to Recurrent networks

Language model: predict the next word

Use MLP naively dosen't handle sequence info well

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-01 17.22.59.png" alt="截屏2021-12-01 17.22.59" style="zoom:50%;" />

## RNN and Gated Rnn

Simple RNN: $ h_t=\phi(W_{hh}h_{t-1}+W_{hx}x_t+b_h)$

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-01 17.30.07.png" alt="截屏2021-12-01 17.30.07" style="zoom:50%;" />

Gated RNN (LSTM, GRU): finer control of information flow

- Forget input: suppress **x~t~** when computing **h~t~**
- Forget past: suppress **h~t-1~** when computing **h~t~**

## Code

Implement Simple RNN

```python
W_xh = nn.Parameter(torch.randn(num_inputs, num_hiddens) * 0.01)
W_hh = nn.Parameter(torch.randn(num_hiddens, num_hiddens) * 0.01)
b_h = nn.Parameter(torch.zeros(num_hiddens))

H = torch.zeros(num_hiddens)
outputs = []

for X in inputs: 
  # inputs shape : (num_steps, batch_size, num_inputs)
  H = torch.tanh(X @ W_xh + H @ W_hh + b_h)
  outputs.append(H)
```

## Bi-RNN and Deep RNN

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-01 17.51.34.png" alt="截屏2021-12-01 17.51.34" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-01 17.53.33.png" alt="截屏2021-12-01 17.53.33" style="zoom:50%;" />

## Model Selections

Tabular

- Trees
- Linear/MLP

Text/Speech

- RNNs
- Transformers

Images/Audio/Video

- Transformers
- CNNs
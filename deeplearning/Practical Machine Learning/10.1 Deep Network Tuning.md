# 10.1 Deep Network Tuning

DL is a programming language to extract information from data

- Some values will be filled by data later
- Differentiable

Various design patterns, from layers to network architecture

Here we talk about some of them

# Batch and Layer Normalization

## Batch Normalization

Standardizing data makes the loss smother for linear methods

- Smooth: <img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-01-04 14.21.42.png" alt="截屏2022-01-04 14.21.42" style="zoom:50%;" />

- A samller beta allows a larger learning rate
- Does not help deep NN

Batch Normalization (BN) standards inputs for internal layers

- Improves the smoothness to make training easier
- (Still controversial why BN works)

### ==Reshape==

Input **X** into 2D (no change for 2D input **X** in R^n*p)

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-01-04 14.27.53.png" alt="截屏2022-01-04 14.27.53" style="zoom:50%;" />

### ==Normalize==

By standardization each column

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-01-04 14.29.23.png" alt="截屏2022-01-04 14.29.23" style="zoom:50%;" />

### ==Recovery==

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2022-01-04 14.30.32.png" alt="截屏2022-01-04 14.30.32" style="zoom:50%;" />

### ==Output==

Output Y by reshaping Y' tothe same shape as X

## Batch Normalization Code

```python
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
  	if not torch.is_grad_enabled(): # In prediction mode
    	X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
  	else:
      	assert len(X.shape) in (2, 4)
      	if len(X.shape) == 2:
        		mean = X.mean(dim=0)
        		var = ((X - mean) ** 2).mean(dim=0)
       	else:
          	mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        X_hat = (X - mean) / torch.sqrt(var + eps)
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta
    return Y, moving_mean, moving_var
```

## Layer Normalization

If apply to RNN, BN needs maintain separated moving statistics for each time step

- Problematic for very long sequences during inference

Layer normalization reshapes input


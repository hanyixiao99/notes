# 28 批量归一化

<!--固定小批量中的均值和方差，然后学习出适合的偏移和缩放-->

<!--可以加速收敛速度（允许更大学习率），但一般不改变模型精度-->

几乎所有的主流卷积神经网络都在用，特别是要做很深的神经网络的时候

## 问题

神经网络很深时，损失出现在最后，后面的层训练较快（梯度大）

数据在最底部

- 底部的层训练比较慢
- 底部层一变化，所有都得跟着变
- 最后的那些层需要重新学习多次
- 收敛变慢

如何在学习底部层时避免变化顶部层

## 批量归一化

固定小批量里面的均值和方差

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-13 10.58.08.png" alt="截屏2021-12-13 10.58.08" style="zoom:50%;" />

然后再做额外的调整（可学习的参数）

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-13 10.59.42.png" alt="截屏2021-12-13 10.59.42" style="zoom:50%;" />

## 批量归一化层

可学习的参数为 $\gamma$ 和 $\beta$

作用在

- 全连接层和卷积层的输出上，激活函数前，进行线性变换
- 全连接层和卷积层的输入上

对全连接层，作用在特征维

对卷积层，作用在通道维

## 批量归一化在做什么

最初论文是想用它来减少内部协变量转移

- 用今天的数据拟合明天，比如疫情前数据去拟合疫情后，分布其实是不同的，模型会有问题

后续有论文指出它可能就是通过在每个小批量里加入噪音来控制模型复杂度

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-13 11.15.06.png" alt="截屏2021-12-13 11.15.06" style="zoom:50%;" />

- 随机偏移miu与随机缩放sigma

因此没必要与丢弃法混合使用

- 都为控制模型复杂度的方法

## 代码实现

```python
import torch
from torch import nn
from d2l import torch as d2l


def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # 通过is_grad_enabled来判断当前模式是训练模式还是预测模式
    if not torch.is_grad_enabled():
        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # 使用全连接层的情况，计算特征维上的均值和方差
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。
            # 这里我们需要保持X的形状以便后面可以做广播运算
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # 训练模式下，用当前的均值和方差做标准化
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 更新移动平均的均值和方差
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # 缩放和移位
    return Y, moving_mean.data, moving_var.data
```

 

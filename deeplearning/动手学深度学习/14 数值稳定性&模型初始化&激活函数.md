# 14 数值稳定性&模型初始化&激活函数

## 神经网络的梯度

考虑有如下d层的神经网络

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 11.00.08.png" alt="截屏2021-12-08 11.00.08" style="zoom:50%;" />

计算损失函数L关于参数**W~t~**的梯度

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 11.00.44.png" alt="截屏2021-12-08 11.00.44" style="zoom:50%;" />

## 数值稳定性的两个常见问题

梯度爆炸与梯度消失

### 例子：MLP

加入如下MLP（为了简单省略了偏移）

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 11.10.43.png" alt="截屏2021-12-08 11.10.43" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 11.10.52.png" alt="截屏2021-12-08 11.10.52" style="zoom:50%;" />

### 梯度爆炸

使用ReLU作为激活函数

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 12.00.48.png" alt="截屏2021-12-08 12.00.48" style="zoom:50%;" />

如果d-t很大，值将会很大，因为上面MLP例子中的函数的一些元素会来自于激活函数导数等于1的那部分

### 梯度爆炸的问题

值超出值域（infinity）

- 对于16位浮点数尤为严重（数值区间6e-5 - 6e4）

对学习率敏感

- 如果学习率太大导致大参数值，从而导致更大的梯度
- 如果学习率太小会导致训练无进展
- 可能需要在训练过程中不断调整学习率

### 梯度消失

使用sigmoid作为激活函数

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 12.07.41.png" alt="截屏2021-12-08 12.07.41" style="zoom:50%;" />

MLP例子中函数的元素值是d-t个小数值的乘积

### 梯度消失的问题

梯度值变成0

- 对16位浮点数尤为严重

训练没有进展

- 无论如何选择学习率

对于底部层尤为严重

- 仅仅顶部层训练的较好
- 无法让神经网络更深

## 让训练更加稳定

目标：让梯度值在合理的范围内

- 例如，[1e-6,1e3]

将乘法变加法

- ResNet，LSTM

归一化

- 梯度归一化，梯度裁剪

合理的初始权重和激活函数

## 让每层的方差是一个常数

将每层的输出和梯度都看作随机变量

让它们的均值和方差都保持一致

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 12.23.45.png" alt="截屏2021-12-08 12.23.45" style="zoom:50%;" />

## 权重初始化

在合理值区间里随机初始参数

因为训练开始的时候更容易有数值不稳定

- 远离最优解的地方损失函数表面可能很复杂
- 最优解附近表面会比较平坦

使用 N~(0,0.01) 来初始可能对小网络来说没问题，但是不能保证深度神经网络

### 例子：MLP

假设

- $w_{i,j}^t$ 是i.i.d（独立同分布），那么E[w]=0，Var[w]=$\gamma_t$
- $h_i^{t-1}$ 独立于 $w_{i,j}^t$

假设没有激活函数 $\pmb{h}^t=\pmb{W}^t\pmb{h}^{t-1}$ ，这里 $\pmb{W}^t\in\mathbb{R}^{n_t\times n_{t-1}}$

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 12.35.41.png" alt="截屏2021-12-08 12.35.41" style="zoom:50%;" />

### 正向方差

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 12.42.13.png" alt="截屏2021-12-08 12.42.13" style="zoom:50%;" />

 ### 反向均值和方差

和正向情况类似

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 13.07.51.png" alt="截屏2021-12-08 13.07.51" style="zoom:50%;" />

## Xavier 初始化

难以同时满足 $n_{t-1}\gamma_t=1$ 和 $n_{t}\gamma_t=1$，因为n~t-1~（第t层输入维度）和n~t~（第t层输出维度）是难以控制的，除非输入刚好等于输出，否则无法同时满足两个条件

Xavier使得 $\gamma_t(n_{t-1}+n_t)/2=1\quad\to \gamma_t=2/(n_{t-1}+n_t)$

- 正态分布 N~(0,$\sqrt{2/(n_{t-1}+n_t)}$ )
- 均匀分布 U~( $-\sqrt{6/(n_{t-1}+n_t)},\sqrt{6/(n_{t-1}+n_t)}$ )
  - 分布 U~[-a,a]的方差是 a^2^/3

适配权重形状变换，特别是n~t~

## 假设线性的激活函数

不会用线性的激活函数，仅为了理论分析方便

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 13.21.27.png" alt="截屏2021-12-08 13.21.27" style="zoom:50%;" />

### 反向

得到同样结论

## 检查常用激活函数

使用==泰勒展开==

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 13.24.08.png" alt="截屏2021-12-08 13.24.08" style="zoom:50%;" />

调整sigmoid

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 13.29.11.png" alt="截屏2021-12-08 13.29.11" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-08 13.29.50.png" alt="截屏2021-12-08 13.29.50" style="zoom:50%;" />
# 07 自动求导

## 向量链式法则

标量链式法则
$$
y=f(u),u=g(x)\qquad  \frac{\partial y}{\partial x}=\frac{\partial y}{\partial u}\frac{\partial u}{\partial x}
$$
拓展到向量
$$
\frac{\partial y}{\partial \pmb{x}}=\frac{\partial y}{\partial u}\frac{\partial u}{\partial \pmb{x}} \qquad \qquad\frac{\partial y}{\partial \pmb{x}}=\frac{\partial y}{\partial \pmb{u}}\frac{\partial \pmb{u}}{\partial \pmb{x}} \qquad \qquad \frac{\partial \pmb{y}}{\partial \pmb{x}}=\frac{\partial \pmb{y}}{\partial \pmb{u}}\frac{\partial \pmb{u}}{\partial \pmb{x}}
\\
(1,n) (1,)(1,n)\qquad\qquad(1,n) (1,k)(k,n)\qquad(m,n) (m,k)(k,n)
$$

## 例子1

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-05 21.03.29.png" alt="截屏2021-12-05 21.03.29" style="zoom:50%;" />

## 例子2

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-05 21.05.01.png" alt="截屏2021-12-05 21.05.01" style="zoom:50%;" />

## 自动求导

自动求导计算一个函数在指定值上的导数

它有别与

- 符号求导
- 数值求导

## 计算图

- 将代码分解成操作子
- 将计算表示成一个无环图

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-05 21.08.03.png" alt="截屏2021-12-05 21.08.03" style="zoom:50%;" />

- 显式构造

  - Tensorflow/Theano/MXNet

  ```python
  from mxnet import sym
  
  a = sym.var()
  b = sym.var()
  c = 2 * a + b
  ```

- 隐式构造

  - PyTorch/MXNet

  ```python
  from mxnet import autograd, nd
  
  with autograd.record():
    a = nd.ones((2,1))
    b = nd.ones((2,1))
    c = 2 * a + b
  ```

## 自动求导的两种模式

链式法则
$$
\frac{\partial y}{\partial x}=\frac{\partial y}{\partial u_n}\frac{\partial u_n}{\partial u_{n-1}}...\frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x}
$$
正向累计
$$
\frac{\partial y}{\partial x}=\frac{\partial y}{\partial u_n}(\frac{\partial u_n}{\partial u_{n-1}}(...\frac{\partial u_2}{\partial u_1}\frac{\partial u_1}{\partial x})))
$$
反向累计（反向传递）
$$
\frac{\partial y}{\partial x}=(((\frac{\partial y}{\partial u_n}\frac{\partial u_n}{\partial u_{n-1}})...)\frac{\partial u_2}{\partial u_1})\frac{\partial u_1}{\partial x}
$$

## 反向累计

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-05 21.19.43.png" alt="截屏2021-12-05 21.19.43" style="zoom:50%;" />

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-05 21.20.45.png" alt="截屏2021-12-05 21.20.45" style="zoom:50%;" />

## 复杂度

计算复杂度：O(n)，n是操作子个数

- 通常正向和反向的代价类似

内存复杂度：O(n)，因为需要存储正向的所有中间结果

跟正向累计对比：

- O(n)计算复杂度用来计算一个变量的梯度
- O(1)内存复杂度

## 自动求导实现

假设我们想对函数 $y=2\pmb{x}^T\pmb{x}$ 关于列向量 $\pmb{x}$ 求导

```python
x = torch.arange(4.0)
```

在我们计算 y 关于 $\pmb{x}$ 的梯度之前，我们需要一个地方来存储梯度

```python
x.requires_grad_(True)
# x = torch.arange(4.0, requires_grad=True)
x.grad
# None
```

现在计算y

```python
y = 2 * torch.dot(x, x)
# tensor(28., grad_fn=<MulBackward0>)
```

通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度

```python
y.backward()
x.grad
# tensor([ 0.,  4.,  8., 12.])
x.grad == 4 * x
# tensor([True, True, True, True])
```

现在计算`x`的另一个函数

在默认情况下，PyTorch会累积梯度，我们需要清除之前的值

```python
x.grad.zero_()
y = x.sum()
y.backward()
x.grad
# tensor([1., 1., 1., 1.])
```

深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和

对非标量调用`backward`需要传入一个`gradient`参数

```python
x.grad.zero_()
y = x * x
# y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
# tensor([0., 2., 4., 6.])
```

将某些计算移动到记录的计算图之外

```python
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u
# tensor([True, True, True, True])
```

即使构建函数的计算图需要通过Python控制流（条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度

```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = b * 2
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b

    return c

a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()

a.grad == d / a
# tensor(True)
```


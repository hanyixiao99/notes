# 08 线性回归基础优化算法

## 线性回归

一个简化模型

- 假设1:影响房价的关键因素是卧室个数、卫生间个数和居住面积，记为 $x_1,x_2,x_3$

- 假设2:成交价是关键因素的加权和
  $$
  y=w_1x_1+w_2x_2+w_3x_3+b
  $$

## 线性模型

给定 $n$ 维输入 $\pmb{x} = [x_1,x_2,...,x_n]^T$

线性模型有一个 $n$ 维权重和一个标量偏差
$$
\pmb{w} = [w_1,w_2,...,w_n]^T, \quad b
$$
输出是输入的加权和
$$
y = w_1x_1+w_2x_2+...+w_nx_n+b
$$
向量版本
$$
y = \left \langle \pmb{w},\pmb{x} \right \rangle + b
$$

## 线性模型可以看作是单层神经网络

Input layer: $x_1,x_2,...,x_d$

Output layer: $O_1$

## 衡量预估质量

比较真实值和预估值，例如房屋售价和估价

假设 $y$ 是真实值，$\hat{y}$ 是估计值，我们可以比较
$$
L(y,\hat{y})= \frac {1}{2}(y-\hat{y})^2
$$
这个叫做平方损失

## 训练数据

收集一些数据点来决定参数值（权重和偏差），例如过去6个月卖的房子，这些数据被称之为==训练数据==，通常越多越好

假设我们有 $n$ 个样本，记
$$
\pmb{X} = [\pmb{x_1},\pmb{x_2},...,\pmb{x_n}]^T \quad \pmb{y} = [y_1,y_2,...,y_n]^T
$$

## 参数学习

训练损失函数
$$
L(\pmb{X},\pmb{y},\pmb{w},b)=\frac{1}{2n}\sum_{i=1}^{n}(y_i-\left \langle \pmb{x_i},\pmb{w} \right \rangle-b)^2=\frac{1}{2n}\left \| \pmb{y}-\pmb{X}\pmb{w}-b \right \|^2
$$
最小化损失函数来学习参数
$$
\pmb{w}^*,b^*=arg\min_{\pmb{w},b}L(\pmb{X},\pmb{y},\pmb{w},b)
$$

## 显式解

将偏差加入权重     $\pmb{X}\leftarrow [\pmb{X},\pmb{1}]\quad \pmb{w}\leftarrow\begin{bmatrix}
\pmb{w}\\ 
b
\end{bmatrix} $
$$
L(\pmb{X},\pmb{y},\pmb{w})=\frac{1}{2n}\left \| \pmb{y}-\pmb{X}\pmb{w}\right \|^2
$$
损失是凸函数，所以最优解满足
$$
\pmb{w}^*=(\pmb{X}^T\pmb{X})^{-1}\pmb{X}^T\pmb{y}
$$

## 基础优化方法

### 梯度下降

挑选一个初始值 $\pmb{w}_0$，重复迭代参数t=1,2,3
$$
\pmb{w}_t=\pmb{w}_{t-1}-\eta\frac{\partial L}{\partial \pmb{w}_{t-1}}
$$

- 沿梯度方向将增加损失函数值
- ==学习率==：步长的超参数

## 小批量随机梯度下降

在整个训练集上算梯度很贵，一个深度神经网络模型可能需要数分钟至数小时

我们可以随机采样b个样本，$i_1,i_2,...,i_b$ 来近似损失
$$
\frac{1}{b}\sum_{i\in I_b}L(\pmb{x}_i,y_i,\pmb{w})
$$

- 其中b是==批量大小==，另一个重要的超参数

## 线性回归的从零开始实现

```python
import random
import torch
from d2l import torch as d2l
```

根据带有噪声的线性模型构造一个人造数据集

使用线性模型参数 $\pmb{w} = [2,-3.4]^T$ 、$b=4.2$ 和噪声项 $\varepsilon $ 生成数据集及其标签：
$$
\pmb{y} =\pmb{X}\pmb{w}+b+\varepsilon 
$$

```python
def synthetic_data(w, b, num_examples):
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2

features, labels = synthetic_data(true_w, true_b, 1000)
```

`features`中的每一行都包含一个二维数据样本，`labels`中的每一行都包含一维标签值（一个标量）

```python
print('features:', features[0], '\nlabel:', labels[0])
# features: tensor([-2.7446,  1.4079]) 
# label: tensor([-6.0943])
```

```python
d2l.set_figsize()
d2l.plt.scatter(features[:, 1].detach().numpy(),
                labels.detach().numpy(), 1)
```

![image-20211206153440629](/Users/hanyixiao/Library/Application Support/typora-user-images/image-20211206153440629.png)

```python
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(indices[i:min(
          i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]


batch_size = 10

for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break

# tensor([[ 1.5362,  0.2872],
#        [ 0.8480, -0.9697],
#        [-1.5358, -0.6041],
#        [ 1.3032,  1.6495],
#        [-0.4856, -0.0249],
#        [ 1.1123, -0.3708],
#        [ 3.0997,  0.8911],
#        [ 0.3333, -0.6150],
#        [ 1.2902, -0.6155],
#        [ 1.7740, -1.3369]]) 
# tensor([[ 6.2940],
#        [ 9.1937],
#        [ 3.1605],
#        [ 1.1942],
#        [ 3.3201],
#        [ 7.7055],
#        [ 7.3778],
#        [ 6.9558],
#        [ 8.8681],
#        [12.2860]])
```

定义初始化模型参数

```python
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

定义模型

```python
def linreg(X, w, b):
  	"""线性回归模型"""
    return torch.matmul(X, w) + b
```

定义损失函数

```python
def squared_loss(y_hat, y):
  	"""均方损失函数"""
    return (y_hat - y.reshape(y_hat.shape))**2 / 2
```

定义优化算法

```python
def sgd(params, lr, batch_size):
  	"""小批量随机梯度下降"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

训练过程

```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y) # X和y的小批量损失
        # 因为l形状是(batch_size, 1)，而不是一个标量
        # l种的所有元素被加到一起
        # 并以此计算关于[w, b]的梯度
        l.sum().backward()
        sgd([w, b], lr, batch_size) # 使用参数的梯度更新
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.m)}')

# epoch 1, loss 0.035542
# epoch 2, loss 0.000129
# epoch 3, loss 0.000050
```

通过比较真实参数和通过训练学习到的参数来评估训练的成功程度

```python
print(f'w的估计误差：{true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差：{true_b - b}')

# w的估计误差：tensor([ 0.0007, -0.0007], grad_fn=<SubBackward0>)
# b的估计误差：tensor([-0.0006], grad_fn=<RsubBackward1>)
```

## 线性回归的简洁实现

通过使用深度学习框架来简洁地实现==线性回归模型==生成数据集

```python
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)
```

调用框架中现有的API来读取数据

```python
def load_array(data_arrays, batch_size, is_train=True):
  	"""构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)

next(iter(data_iter))

# [tensor([[ 0.7443, -0.1291],
#          [-0.6561,  0.0714],
#          [-0.7611, -0.0123],
#          [-1.2788,  0.0438],
#          [-0.8965,  0.5494],
#          [ 0.4094,  1.3473],
#          [-1.2694,  0.5520],
#          [-0.2612,  1.1155],
#          [-1.3967,  0.4226],
#          [-0.7713,  0.4189]]), 
#  tensor([[ 6.1579],
#          [ 2.6698],
#          [ 2.7138],
#          [ 1.4841],
#          [ 0.5412],
#          [ 0.4334],
#          [-0.2170],
#          [-0.1051],
#          [-0.0139],
#          [ 1.2234]])]
```

使用框架预定义好的层

```python
from torch import nn # nn 是神经网络的缩写

net = nn.Sequential(nn.Linear(2, 1))
```

初始化模型参数

```python
net[0].weight.data.normal_(0, 0.01) # 使用正态分布替换normal的值
net[0].bias.data.fill_(0)
```

计算均方误差使用的是`MSELoss`类，也称为 L~2~ 平方范数

```python
loss = nn.MSELoss()
```

实例化`SGD`实例

```python
trainer = torch.optim.SGD(net.parameters(), lr = 0.03)
```

训练过程代码与从零开始实现时所做的非常相似

```python
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X), y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

# epoch 1, loss 0.000251
# epoch 2, loss 0.000097
# epoch 3, loss 0.000097
```


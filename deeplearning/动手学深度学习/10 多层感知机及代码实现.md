# 10 多层感知机及代码实现

## 感知机

给定输入 $\pmb{x}$ ，权重 $\pmb{w}$ 和偏移b，感知机输出：
$$
o=\sigma(\left \langle \pmb{w},\pmb{x} \right \rangle+b)\quad \sigma(x)=\left\{\begin{matrix}
1 &\mathrm{\mathrm{if}}\ x>0\\ 
-1 &\mathrm{\mathrm{otherwise}}
\end{matrix}\right.
$$
<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-06 23.20.07.png" alt="截屏2021-12-06 23.20.07" style="zoom:50%;" />

 二分类：1或-1

- Vs. 线性回归输出实数
- Vs. Softmax回归输出概率

## 训练感知机（1960s）

> **initialize** w = 0 and b = 0
> **repeat**
> 	**if** $y_i[\left \langle w,x \right \rangle+b]\leq0$ **then**   // 预测与实际相反，二者异号，分类错误
>
> ​		$w\leftarrow w+y_ix_i$ and $b\leftarrow b+y_i$
>
> ​	**end if**
>
> **until** all classified correctly

等价于使用批量大小为1的梯度下降，并使用如下的损失函数
$$
L(y,\pmb{x},\pmb{w})=\mathrm{\mathrm{max}}(0,-y\left \langle \pmb{w},\pmb{x} \right \rangle)
$$
其中max0对应if语句，如果分类正确则输出为0，如果分类错误则产生梯度，进入if语句进行更新

## 收敛定理

假设数据在半径 $r$ 内，余量 $\rho$ 分类两类
$$
y(\pmb{x}^T\pmb{w}+b)\geq\rho
$$
对于 $\left \|\pmb{w}  \right \|^2+b^2\leq1$，感知机保证在  $\frac{r^2+1}{\rho^2}$ 步后收敛

## XOR问题 (Minsky & Papert, 1969)

 感知机不能拟合XOR函数，它只能产生线性分割面

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-06 23.56.38.png" alt="截屏2021-12-06 23.56.38" style="zoom:50%;" />

## 学习XOR



<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-07 00.01.15.png" alt="截屏2021-12-07 00.01.15" style="zoom:50%;" />

## 单隐藏层

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-07 00.02.41.png" alt="截屏2021-12-07 00.02.41" style="zoom:67%;" />

隐藏层的大小是超参数

### 单隐藏层—单分类

输入 $\pmb{x}\in\mathbb{R}^n$

隐藏层 $\pmb{W_1}\in\mathbb{R}^{m\times n},\pmb{b_1}\in\mathbb{R}^n$

输出层 $\pmb{w_2}\in\mathbb{R}^m,b_2\in\mathbb{R}$
$$
\pmb{h}=\sigma(\pmb{W_1x}+\pmb{b_1})\\o=\pmb{w}^T_2\pmb{h}+b_2\quad\
$$
其中 $\sigma$ 是按元素做运算的==非线性激活函数==，如果不是非线性激活函数，输出o仍然是线性，此时等价于单层感知机

## 激活函数

### Sigmoid激活函数

将输入投影到（0，1），较为soft的$\sigma(x)=\left\{\begin{matrix}
1 &\mathrm{\mathrm{if}}\ x>0\\ 
0 &\mathrm{\mathrm{otherwise}}
\end{matrix}\right.$
$$
\mathrm{\mathrm{sigmoid}}(x)=\frac{1}{1+\mathrm{\mathrm{exp}}(-x)}
$$
<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-07 11.03.58.png" alt="截屏2021-12-07 11.03.58" style="zoom:50%;" />

### Tanh激活函数

将输入投影到（-1，1），同样是soft版本
$$
\mathrm{\mathrm{tanh}}(x)=\frac{1-\mathrm{\mathrm{exp}}(-2x)}{1+\mathrm{\mathrm{exp}}(-2x)}
$$
<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-07 11.19.26.png" alt="截屏2021-12-07 11.19.26" style="zoom:50%;" />

### ReLU激活函数

ReLU: rectified linear unit 
$$
\mathrm{\mathrm{ReLU}}(x)=\mathrm{\mathrm{max}}(x,0)
$$
<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-07 11.22.03.png" alt="截屏2021-12-07 11.22.03" style="zoom:50%;" />

## 多类分类

$$
y_1,y_2,...,y_k=\mathrm{\mathrm{softmax}}(o_1,o_2,...,o_k)
$$

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/截屏2021-12-07 11.27.19.png" alt="截屏2021-12-07 11.27.19" style="zoom:50%;" />

多类分类与softmax回归区别是中间多了Hidden layer，加了Hidden layer变成多层感知机

输入 $\pmb{x}\in\mathbb{R}^n$

隐藏层 $\pmb{W_1}\in\mathbb{R}^{m\times n},\pmb{b_1}\in\mathbb{R}^m$

输出层 $\pmb{W_2}\in\mathbb{R}^{m\times k},\pmb{b_2}\in\mathbb{R}^k$
$$
\begin{aligned}
\pmb{h}&=\sigma(\pmb{W_1x}+\pmb{b_1})\\ \pmb{o}&=\pmb{w}^T_2\pmb{h}+\pmb{b_2}\\ \pmb{y}&=\mathrm{\mathrm{softmax}}(\pmb{o})
\end{aligned}
$$

## 多隐藏层

$$
\begin{aligned}
\pmb{h_1}&=\sigma(\pmb{W_1x}+\pmb{b_1})\\ \pmb{h_2}&=\sigma(\pmb{W_2h_1}+\pmb{b_2})\\ \pmb{h_3}&=\sigma(\pmb{W_3h_2}+\pmb{b_3})\\
\pmb{o}&=\pmb{w_4h_3}\pmb{h}+\pmb{b_4}
\end{aligned}
$$

超参数：隐藏层数以及每层隐藏层的大小

## 多层感知机总结

多层感知机使用隐藏层和激活函数来得到非线性模型

使用Softmax来处理多分类

## 多层感知机的从零开始实现

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

实现一个具有单隐藏层的多层感知机，它包含256个隐藏单元

```python
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))
W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]
```

实现ReLU激活函数

```python
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
```

实现模型

```python
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X @ W1 + b1)
    return (H @ W2 + b2)

loss = nn.CrossEntropyLoss()
```

多层感知机的训练过程与softmax回归的训练过程完全相同

```python
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater) 
```

## 多层感知机的简洁实现

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

隐藏层包含256个隐藏单元，并使用了ReLU激活函数

```python
net = nn.Sequential(
  nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)
        
net.apply(init_weights)
```

训练过程

```python
batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss()
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

<img src="/Users/hanyixiao/Library/Application Support/typora-user-images/image-20211207131236725.png" alt="image-20211207131236725" style="zoom:50%;" />
